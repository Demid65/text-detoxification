from transformers import T5Tokenizer, T5ForConditionalGeneration
import os
import sys
import warnings
import argparse

MODEL_PATH = 'models/t5-small-detoxification'

PREFIX = 'detoxify text: '
MAX_NEW_TOKENS = 128

INPUT = 'stdin'
OUTPUT = 'stdout'

parser = argparse.ArgumentParser(
                    prog='python .\src\data\split_dataset.py',
                    description='splits the dataset generated by download_dataset.py scripy.',
                    epilog='https://github.com/Demid65/text-detoxification')

parser.add_argument('--model', type=str, metavar='MODEL_PATH', dest='MODEL_PATH',
                    help=f'Path to the trained model, should be a T5 model. Could be a hugging face model identifier. Defaults to {MODEL_PATH}', default=MODEL_PATH)
                    
parser.add_argument('--prefix', type=str, metavar='PREFIX', dest='PREFIX',
                    help=f'Prefix that is added to every model input. Defaults to "{PREFIX}"', default=PREFIX)

parser.add_argument('--max_new_tokes', type=int, metavar='MAX_NEW_TOKENS', dest='MAX_NEW_TOKENS',
                    help=f'Maximum number of tokens model gets to generate. Defaults to {MAX_NEW_TOKENS}', default=MAX_NEW_TOKENS)                            

parser.add_argument('--input', type=str, metavar='INPUT_FILE', dest='INPUT',
                    help=f'Path to input file. If set to stdin, takes input from the console. Defaults to {INPUT}', default=INPUT)  

parser.add_argument('--output', type=str, metavar='OUTPUT_FILE', dest='OUTPUT',
                    help=f'Path to output file. If set to stdout, sends output to the console. Defaults to {OUTPUT}', default=OUTPUT)  

args = parser.parse_args()

MODEL_PATH = args.MODEL_PATH
PREFIX = args.PREFIX
MAX_NEW_TOKENS = args.MAX_NEW_TOKENS
INPUT = args.INPUT
OUTPUT = args.OUTPUT

# load model
print(f'loading model from {MODEL_PATH}')
tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)
model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)

if INPUT == 'stdin':
    input_file = sys.stdin
else: 
    input_file = open(INPUT)

if OUTPUT == 'stdout':
    out_file = sys.stdout
else: 
    out_file = open(OUTPUT, 'w')

if INPUT == 'stdin':
    print('ready')

while True:
    line = input_file.readline()
    if not line:
        break
    input_ids = tokenizer(PREFIX + line, return_tensors="pt").input_ids
    outputs = model.generate(input_ids, max_new_tokens = MAX_NEW_TOKENS)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)

    out_file.write(result + os.linesep)
 
input_file.close()
out_file.close()
print('done')

