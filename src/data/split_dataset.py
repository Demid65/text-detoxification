import numpy as np
import pandas as pd
from datasets import Dataset
from sklearn.model_selection import train_test_split
import os
import warnings
import argparse

warnings.filterwarnings("ignore")

INTERIM_PATH = 'data/interim/processed.csv'

TRAINING_DATA_FOLDER = 'data/training/'

NUM_PRETRAIN = 20000
NUM_PRETRAIN_VAL = 5000
NUM_TRAIN = 150000
NUM_TRAIN_VAL = 10000

# parse the command line arguments
parser = argparse.ArgumentParser(
                    prog='python .\src\data\split_dataset.py',
                    description='splits the dataset generated by download_dataset.py scripy.',
                    epilog='https://github.com/Demid65/text-detoxification')

parser.add_argument('--load_from', type=str, metavar='INPUT_FILE', dest='INTERIM_PATH',
                    help=f'Path where processed dataset is stored. Defaults to {INTERIM_PATH}', default=INTERIM_PATH) 
                    
parser.add_argument('--save_to', type=str, metavar='TRAINING_DATA_FOLDER', dest='TRAINING_DATA_FOLDER',
                    help=f'Folder to store the split datasets. Defaults to {TRAINING_DATA_FOLDER}', default=TRAINING_DATA_FOLDER)

parser.add_argument('--num_pretrain', type=int, metavar='NUM_PRETRAIN', dest='NUM_PRETRAIN',
                    help=f'Number of training samples for pretraining stage. Defaults to {NUM_PRETRAIN}', default=NUM_PRETRAIN)

parser.add_argument('--num_pretrain_val', type=int, metavar='NUM_PRETRAIN_VAL', dest='NUM_PRETRAIN_VAL',
                    help=f'Number of training samples for pretraining validation. Defaults to {NUM_PRETRAIN_VAL}', default=NUM_PRETRAIN_VAL)                            

parser.add_argument('--num_train', type=int, metavar='NUM_TRAIN', dest='NUM_TRAIN',
                    help=f'Number of training samples for main training stage. Defaults to {NUM_TRAIN}', default=NUM_TRAIN)  

parser.add_argument('--num_train_val', type=int, metavar='NUM_TRAIN_VAL', dest='NUM_TRAIN_VAL',
                    help=f'Number of training samples for main training validation. Defaults to {NUM_TRAIN_VAL}', default=NUM_TRAIN_VAL)

args = parser.parse_args()

INTERIM_PATH = args.INTERIM_PATH
TRAINING_DATA_FOLDER = args.TRAINING_DATA_FOLDER
NUM_PRETRAIN = args.NUM_PRETRAIN
NUM_PRETRAIN_VAL = args.NUM_PRETRAIN_VAL
NUM_TRAIN = args.NUM_TRAIN
NUM_TRAIN_VAL = args.NUM_TRAIN_VAL

# load the data
print(f'loading processed dataset from {INTERIM_PATH}')
df = pd.read_csv(INTERIM_PATH)
df = df[['toxic','detoxified']].rename(columns={'toxic':'input','detoxified':'target'})

# split the datasets
temp, pretrain = train_test_split(df, test_size=NUM_PRETRAIN / len(df), random_state=42)
temp, pretrain_val = train_test_split(temp, test_size=NUM_PRETRAIN_VAL / len(temp), random_state=42)
temp, train = train_test_split(temp, test_size=NUM_TRAIN / len(temp), random_state=42)
test, train_val = train_test_split(temp, test_size=NUM_TRAIN_VAL / len(temp), random_state=42)

# convert datasets into huggingface format
pretrain_dataset = Dataset.from_dict(pretrain.to_dict(orient='list'))
pretrain_val_dataset = Dataset.from_dict(pretrain_val.to_dict(orient='list'))
train_dataset = Dataset.from_dict(train.to_dict(orient='list'))
train_val_dataset = Dataset.from_dict(train_val.to_dict(orient='list'))
test_dataset = Dataset.from_dict(test.to_dict(orient='list'))

# save datasets on disk
print(f'saving split datasets into {TRAINING_DATA_FOLDER}')
pretrain_dataset.save_to_disk(os.path.join(TRAINING_DATA_FOLDER, 'pretrain'))
pretrain_val_dataset.save_to_disk(os.path.join(TRAINING_DATA_FOLDER, 'pretrain_val'))
train_dataset.save_to_disk(os.path.join(TRAINING_DATA_FOLDER, 'train'))
train_val_dataset.save_to_disk(os.path.join(TRAINING_DATA_FOLDER, 'train_val'))
test_dataset.save_to_disk(os.path.join(TRAINING_DATA_FOLDER, 'test'))

print('done')